{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1AGFVYSn49PQZ8OP_hoVWROdjTOBU59M5","authorship_tag":"ABX9TyN7/TdWoX1yqJ0Cds0jDxl9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c3b47cdba9384765b3f2f46611cb7dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4738297226744d46a4b73d376bc45457","IPY_MODEL_8587c9f9091f4335a917b4e896a3c1a4","IPY_MODEL_ed06a36569a849c6baa88ab0cb25cafa"],"layout":"IPY_MODEL_aa0290bf7a4f46039ea92c9cf05c864c"}},"4738297226744d46a4b73d376bc45457":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a1a7855cebd4721bc77a39581e9ead0","placeholder":"​","style":"IPY_MODEL_27fec5f1d8ea42b69d64b2068be97daf","value":"100%"}},"8587c9f9091f4335a917b4e896a3c1a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7acebe51ce44938a14ece039b68e449","max":153559,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86f5de33c15e46f6a5baa0822af4e161","value":153559}},"ed06a36569a849c6baa88ab0cb25cafa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b84315ec0324813bfa81fe5eb7d8d56","placeholder":"​","style":"IPY_MODEL_fe9e1270d96d469e870d1c7e9968c013","value":" 153559/153559 [00:00&lt;00:00, 329979.22it/s]"}},"aa0290bf7a4f46039ea92c9cf05c864c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a1a7855cebd4721bc77a39581e9ead0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27fec5f1d8ea42b69d64b2068be97daf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7acebe51ce44938a14ece039b68e449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86f5de33c15e46f6a5baa0822af4e161":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b84315ec0324813bfa81fe5eb7d8d56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe9e1270d96d469e870d1c7e9968c013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import joblib\n","import random\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model\n","from keras.utils import to_categorical\n","from keras import Input\n","from tqdm.notebook import tqdm\n","import keras"],"metadata":{"id":"NAxL-3rnfnDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files = os.listdir(\"/content/drive/MyDrive/Video Captioning/Data/Training Videos/feat\")\n","files = files[:300]\n","for i in range(len(files)):\n","  files[i] = files[i][:-8]\n","files"],"metadata":{"id":"e3HbxzkAl3xD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training_df = pd.read_csv(\"/content/drive/MyDrive/Video Captioning/Data/training_captions.csv\")\n","# training_df['captions'] = training_df['captions'].str[:-1]\n","# training_df.head()\n","\n","# training_df.to_csv(\"/content/drive/MyDrive/Video Captioning/Data/training_captions.csv\")"],"metadata":{"id":"CKI5klzzs0W-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIZ-1evb6nc7","executionInfo":{"status":"ok","timestamp":1687033573397,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"938f23c8-ad91-4bc1-d6f5-916d81030337"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["BASE_DIR = \"/content/drive/MyDrive/Video Captioning/Data\"\n","TRAIN_FEATURE_DIR = os.path.join(BASE_DIR,\"Training Videos/feat\")\n","features = {}\n","for file in files:\n","  f = np.load(os.path.join(TRAIN_FEATURE_DIR, file + \".avi.npy\"), allow_pickle=True)\n","  features[file] = f\n","\n","features"],"metadata":{"id":"fdW0nwmSM769"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tB1f4pta6tfq","executionInfo":{"status":"ok","timestamp":1687179356504,"user_tz":-330,"elapsed":78,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"3319305a-e05d-46a2-df7e-691745de1153"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["with open(os.path.join(BASE_DIR, 'training_captions.csv'), 'r') as f:\n","    next(f)\n","    captions_doc = f.read()"],"metadata":{"id":"HG7glfGLm0v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create mapping of image to captions\n","mapping = {}\n","# process lines\n","for line in tqdm(captions_doc.split('\\n')):\n","    #split the line by comma(,)\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    caption[0] = caption[0][1:]\n","    #remove extension from image id\n","    image_id = image_id.split('.')[0]\n","    # convert caption list to string\n","    caption = \" \".join(caption)\n","    # create list if needed\n","    if image_id in files:\n","      if image_id not in mapping:\n","        mapping[image_id] = []\n","      mapping[image_id].append(caption)\n","mapping['9Q0JfdP36kI_56_64']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535,"referenced_widgets":["c3b47cdba9384765b3f2f46611cb7dcf","4738297226744d46a4b73d376bc45457","8587c9f9091f4335a917b4e896a3c1a4","ed06a36569a849c6baa88ab0cb25cafa","aa0290bf7a4f46039ea92c9cf05c864c","7a1a7855cebd4721bc77a39581e9ead0","27fec5f1d8ea42b69d64b2068be97daf","d7acebe51ce44938a14ece039b68e449","86f5de33c15e46f6a5baa0822af4e161","5b84315ec0324813bfa81fe5eb7d8d56","fe9e1270d96d469e870d1c7e9968c013"]},"id":"DNOwl2zVm0mE","executionInfo":{"status":"ok","timestamp":1687179356910,"user_tz":-330,"elapsed":408,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"5cb22788-6ce3-4ebf-dee9-ac07bb3caa3f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/153559 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b47cdba9384765b3f2f46611cb7dcf"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["['a woman peels a small onion or garlic clove with her fingers and then cuts it with a knife',\n"," 'a woman is peeling a clove a garlic',\n"," 'a woman is peeling garlic',\n"," 'a woman is peeling the outer skin off of a small onion',\n"," 'a woman is peeling a garlic',\n"," 'a woman is peeling a clove of garlic',\n"," 'a woman is peeling a garlic',\n"," 'a woman is peeling and mincing a clove of garlic',\n"," 'a woman is peeling a garlic clove',\n"," 'a woman is peeling some garlic',\n"," 'a woman is peeling garlic',\n"," 'the woman is peeling garlic',\n"," 'a woman is peeling and crushing a clove of garlic',\n"," 'a woman peels an onion',\n"," 'a woman is peeling garlic',\n"," 'the lady peeled the garlic',\n"," 'she is doing the something',\n"," 'doing chiken',\n"," 'a woman is peeling garlic',\n"," 'a woman peels garlic',\n"," 'a woman is peeling garlic',\n"," 'a lady making a dish in the kitchen',\n"," 'a woman is peeling some garlic',\n"," 'a japanes woman removing peel of garlic clove and mash it with single hands',\n"," 'the lady peeled the skin off the garlic',\n"," 'the lady is mashing the garlic',\n"," 'a man cooking his kichen',\n"," 'the woman is peeling garlic']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["mapping['9Q0JfdP36kI_56_64']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzDNuqrA67rk","executionInfo":{"status":"ok","timestamp":1687179357486,"user_tz":-330,"elapsed":588,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"af6b6fc7-62fc-41d2-8b9e-5dfd4a29fa1b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a woman peels a small onion or garlic clove with her fingers and then cuts it with a knife',\n"," 'a woman is peeling a clove a garlic',\n"," 'a woman is peeling garlic',\n"," 'a woman is peeling the outer skin off of a small onion',\n"," 'a woman is peeling a garlic',\n"," 'a woman is peeling a clove of garlic',\n"," 'a woman is peeling a garlic',\n"," 'a woman is peeling and mincing a clove of garlic',\n"," 'a woman is peeling a garlic clove',\n"," 'a woman is peeling some garlic',\n"," 'a woman is peeling garlic',\n"," 'the woman is peeling garlic',\n"," 'a woman is peeling and crushing a clove of garlic',\n"," 'a woman peels an onion',\n"," 'a woman is peeling garlic',\n"," 'the lady peeled the garlic',\n"," 'she is doing the something',\n"," 'doing chiken',\n"," 'a woman is peeling garlic',\n"," 'a woman peels garlic',\n"," 'a woman is peeling garlic',\n"," 'a lady making a dish in the kitchen',\n"," 'a woman is peeling some garlic',\n"," 'a japanes woman removing peel of garlic clove and mash it with single hands',\n"," 'the lady peeled the skin off the garlic',\n"," 'the lady is mashing the garlic',\n"," 'a man cooking his kichen',\n"," 'the woman is peeling garlic']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def clean(mapping):\n","  for key, captions in mapping.items():\n","    for i in range(len(captions)):\n","      caption = captions[i]\n","      # preprocessing text\n","      # converting to lower case\n","      caption = caption.lower()\n","      # delete digits and special characters etc,\n","      caption = caption.replace('[A=Za-z]','')\n","      # delete addtional spaces\n","      caption = caption.replace('\\s+', ' ')\n","      # add start and end tags to the caption\n","      caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1])  + ' endseq'\n","      captions[i]= caption\n","    valid_captions = []\n","    # Removing captions that are smaller than 6 words or larger than 12\n","    for caption in captions:\n","      if len(caption.split(\" \")) >= 6 and len(caption.split(\" \")) <= 12:\n","        valid_captions.append(caption)\n","\n","        # Assign the new valid captions list to the original captions list\n","    mapping[key] = valid_captions\n","clean(mapping)\n","mapping['9Q0JfdP36kI_56_64']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZgW9LqNm019","executionInfo":{"status":"ok","timestamp":1687179358995,"user_tz":-330,"elapsed":550,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"a51b0bc0-0426-4fbd-daad-b41039b1584e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['startseq woman is peeling clove garlic endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq woman is peeling the outer skin off of small onion endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq woman is peeling clove of garlic endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq woman is peeling and mincing clove of garlic endseq',\n"," 'startseq woman is peeling garlic clove endseq',\n"," 'startseq woman is peeling some garlic endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq the woman is peeling garlic endseq',\n"," 'startseq woman is peeling and crushing clove of garlic endseq',\n"," 'startseq woman peels an onion endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq the lady peeled the garlic endseq',\n"," 'startseq she is doing the something endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq woman is peeling garlic endseq',\n"," 'startseq lady making dish in the kitchen endseq',\n"," 'startseq woman is peeling some garlic endseq',\n"," 'startseq the lady peeled the skin off the garlic endseq',\n"," 'startseq the lady is mashing the garlic endseq',\n"," 'startseq man cooking his kichen endseq',\n"," 'startseq the woman is peeling garlic endseq']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["mapping"],"metadata":{"id":"_XWU-tdx2RyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(mapping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYl6b7OB2D1y","executionInfo":{"status":"ok","timestamp":1687027040844,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"484d26d5-43eb-4339-d07b-2e179dd3131f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["293"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["lengths = [len(v) for v in mapping.values()]\n","lengths"],"metadata":{"id":"gpK04TltHglJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(lengths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FegL_q2YIat1","executionInfo":{"status":"ok","timestamp":1687015072539,"user_tz":-330,"elapsed":19,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"d2bb28aa-9af7-44cf-ba8d-1ebf36a7550d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10259"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["train_path = \"/content/drive/MyDrive/Video Captioning/Data/Testing Videos\"\n","test_path = \"/content/drive/MyDrive/Video Captioning/Data/Training Videos\"\n","max_length = 12\n","batch_size = 10\n","learning_rate = 0.0007\n","epochs = 150\n","latent_dim = 512\n","num_encoder_tokens = 4096\n","num_decoder_tokens = 1500\n","time_steps_encoder = 80\n","time_steps_decoder = 12\n","max_probability = -1\n","save_model_path = \"/content/drive/MyDrive/Video Captioning/model_final\"\n","validation_split = 0.15\n","search_type = 'greedy'\n","\n","\n","\n","all_captions = []\n","for key in mapping:\n","  for caption in mapping[key]:\n","      all_captions.append(caption)"],"metadata":{"id":"Jt-e-a8evxha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get maximum length of the captions avilable\n","max_length = max(len(caption.split()) for caption in all_captions)\n","print(max_length)\n","min_length = min(len(caption.split()) for caption in all_captions)\n","print(min_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3m8AqMUJvxkl","executionInfo":{"status":"ok","timestamp":1687179364189,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"a048fe7d-5827-4b2e-89a0-b95fb6d70d83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12\n","6\n"]}]},{"cell_type":"code","source":["# Train Test Split\n","image_ids = list(mapping.keys())\n","#print(image_ids)\n","random.shuffle(image_ids)\n","split = int(len(image_ids)*validation_split)\n","training_list = image_ids[split:]\n","validation_list = image_ids[:split]"],"metadata":{"id":"E6HWp-ODvxvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(training_list))\n","print(len(validation_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41-KqYZCLpF_","executionInfo":{"status":"ok","timestamp":1687179365187,"user_tz":-330,"elapsed":1,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"e957c38d-bfba-4b12-95db-c79222162ae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["250\n","43\n"]}]},{"cell_type":"code","source":["from collections import OrderedDict"],"metadata":{"id":"tp6b-0VxAJxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_captions = []\n","\n","for id in training_list:\n","  for caption in mapping[id]:\n","      training_captions.append(caption)\n","\n","tokenizer = Tokenizer(num_words= num_decoder_tokens)\n","tokenizer.fit_on_texts(training_captions)\n","\n","# if len(tokenizer.word_index) > num_decoder_tokens:\n","#     tokenizer.word_index = dict(list(tokenizer.word_index.items())[:num_decoder_tokens])\n","#     tokenizer.num_words = num_decoder_tokens\n","\n","# my_dict = tokenizer.word_counts\n","# print(OrderedDict(sorted(my_dict.items(), key=lambda x: x[1], reverse=True)))\n","\n","vocab_size =len(tokenizer.word_index)+1\n","# tokenizer.word_counts\n","vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufQk_-cKvAs4","executionInfo":{"status":"ok","timestamp":1687179367229,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"0d3727dd-4900-4627-8e9d-50cb756cfa8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3478"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ANDCLhoQLoo0"}},{"cell_type":"code","source":["def data_loader(training_list,mapping,features,tokenizer,max_length,epochs,num_decoder_tokens,batch_size):\n","  encoder_input_data = []\n","  decoder_input_data = []\n","  decoder_target_data = []\n","  videoId = []\n","  videoSeq = []\n","  for id in training_list:\n","    captions = mapping[id]\n","    for caption in captions:\n","      videoId.append(id)\n","      videoSeq.append(caption)\n","\n","  train_sequences = tokenizer.texts_to_sequences(videoSeq)\n","  train_sequences = np.array(train_sequences, dtype=object)\n","  train_sequences = pad_sequences(train_sequences, padding='post', truncating='post',\n","                                  maxlen=max_length)\n","\n","  file_size = len(train_sequences)\n","  n = 0\n","  for i in range(epochs):\n","      for idx in range(0, file_size):\n","          n += 1\n","          encoder_input_data.append(features[videoId[idx]])\n","          y = to_categorical(train_sequences[idx], num_decoder_tokens)\n","          decoder_input_data.append(y[:-1])\n","          decoder_target_data.append(y[1:])\n","          if n == batch_size or idx == file_size-1:\n","              encoder_input = np.array(encoder_input_data)\n","              decoder_input = np.array(decoder_input_data)\n","              decoder_target = np.array(decoder_target_data)\n","              encoder_input_data = []\n","              decoder_input_data = []\n","              decoder_target_data = []\n","              n = 0\n","              yield ([encoder_input, decoder_input], decoder_target)"],"metadata":{"id":"lF45eZzivxyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_path = \"/content/drive/MyDrive/Video Captioning/Data/Testing Videos\"\n","test_path = \"/content/drive/MyDrive/Video Captioning/Data/Training Videos\"\n","max_length = 12\n","batch_size = 10\n","learning_rate = 0.0007\n","epochs = 150\n","latent_dim = 512\n","num_encoder_tokens = 4096\n","num_decoder_tokens = 1500\n","time_steps_encoder = 80\n","time_steps_decoder = 12\n","max_probability = -1\n","save_model_path = \"/content/drive/MyDrive/Video Captioning/model_final\"\n","validation_split = 0.15\n","search_type = 'greedy'\n","\n","def train_model(training_list,validation_list):\n","  \"\"\"\n","  an encoder decoder sequence to sequence model\n","  reference : https://arxiv.org/abs/1505.00487\n","  \"\"\"\n","  encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n","  encoder = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder_lstm')\n","  _, state_h, state_c = encoder(encoder_inputs)\n","  encoder_states = [state_h, state_c]\n","\n","  decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name=\"decoder_inputs\")\n","  decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n","  decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","  decoder_dense = Dense(num_decoder_tokens, activation='relu', name='decoder_relu')\n","  decoder_outputs = decoder_dense(decoder_outputs)\n","\n","  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","  model.summary()\n","\n","\n","  train = data_loader(training_list,mapping,features,tokenizer,max_length,epochs,num_decoder_tokens,batch_size)\n","  valid = data_loader(validation_list,mapping,features,tokenizer,max_length,epochs,num_decoder_tokens,batch_size)\n","\n","  early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n","\n","  # Run training\n","  opt = keras.optimizers.Adam(lr=0.0003)\n","  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n","                                                factor=0.1, patience=5, verbose=0,\n","                                                mode=\"auto\")\n","  model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n","\n","  validation_steps = len(validation_list)//batch_size\n","  steps_per_epoch = len(training_list)//batch_size\n","\n","  model.fit(train, validation_data=valid, validation_steps=validation_steps,\n","            epochs=epochs, steps_per_epoch=steps_per_epoch,\n","            callbacks=[reduce_lr, early_stopping])\n","\n","  if not os.path.exists(save_model_path):\n","      os.makedirs(save_model_path)\n","\n","  encoder_model = Model(encoder_inputs, encoder_states)\n","  decoder_state_input_h = Input(shape=(latent_dim,))\n","  decoder_state_input_c = Input(shape=(latent_dim,))\n","  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","  decoder_outputs, state_h, state_c = decoder_lstm(\n","      decoder_inputs, initial_state=decoder_states_inputs)\n","  decoder_states = [state_h, state_c]\n","  decoder_outputs = decoder_dense(decoder_outputs)\n","  decoder_model = Model(\n","      [decoder_inputs] + decoder_states_inputs,\n","      [decoder_outputs] + decoder_states)\n","  #encoder_model.summary()\n","  #decoder_model.summary()\n","\n","  # saving the models\n","  encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n","  decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n","  with open(os.path.join(save_model_path, 'tokenizer' + str(num_decoder_tokens)), 'wb') as file:\n","      joblib.dump(tokenizer, file)"],"metadata":{"id":"sHlsR2pUGs8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(training_list,validation_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuP1kMtsGs_H","executionInfo":{"status":"ok","timestamp":1687179406940,"user_tz":-330,"elapsed":36368,"user":{"displayName":"Vidya","userId":"07181660108449302432"}},"outputId":"bbe5a72f-a6ea-433d-96ea-03af4bc1c38b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, 80, 4096)]   0           []                               \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, 12, 1500)]   0           []                               \n","                                                                                                  \n"," encoder_lstm (LSTM)            [(None, 80, 512),    9439232     ['encoder_inputs[0][0]']         \n","                                 (None, 512),                                                     \n","                                 (None, 512)]                                                     \n","                                                                                                  \n"," decoder_lstm (LSTM)            [(None, 12, 512),    4122624     ['decoder_inputs[0][0]',         \n","                                 (None, 512),                     'encoder_lstm[0][1]',           \n","                                 (None, 512)]                     'encoder_lstm[0][2]']           \n","                                                                                                  \n"," decoder_relu (Dense)           (None, 12, 1500)     769500      ['decoder_lstm[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 14,331,356\n","Trainable params: 14,331,356\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","25/25 [==============================] - 9s 72ms/step - loss: 7.1637 - accuracy: 0.3400 - val_loss: 6.2852 - val_accuracy: 0.4364 - lr: 3.0000e-04\n","Epoch 2/150\n","25/25 [==============================] - 1s 37ms/step - loss: 6.9178 - accuracy: 0.4098 - val_loss: 6.0930 - val_accuracy: 0.4455 - lr: 3.0000e-04\n","Epoch 3/150\n","25/25 [==============================] - 1s 36ms/step - loss: 6.7996 - accuracy: 0.3978 - val_loss: 5.4782 - val_accuracy: 0.4977 - lr: 3.0000e-04\n","Epoch 4/150\n","25/25 [==============================] - 1s 37ms/step - loss: 6.9938 - accuracy: 0.3825 - val_loss: 6.5428 - val_accuracy: 0.4318 - lr: 3.0000e-04\n","Epoch 5/150\n","25/25 [==============================] - 1s 36ms/step - loss: 7.1523 - accuracy: 0.4007 - val_loss: 7.7845 - val_accuracy: 0.3500 - lr: 3.0000e-04\n","Epoch 6/150\n","25/25 [==============================] - 1s 36ms/step - loss: 6.0848 - accuracy: 0.4727 - val_loss: 6.2156 - val_accuracy: 0.4477 - lr: 3.0000e-04\n","Epoch 7/150\n","25/25 [==============================] - 1s 49ms/step - loss: 6.9245 - accuracy: 0.4211 - val_loss: 6.5381 - val_accuracy: 0.4818 - lr: 3.0000e-04\n","Epoch 8/150\n","25/25 [==============================] - 1s 44ms/step - loss: 7.2606 - accuracy: 0.4462 - val_loss: 4.4552 - val_accuracy: 0.6477 - lr: 3.0000e-04\n","Epoch 9/150\n","25/25 [==============================] - 1s 43ms/step - loss: 6.7050 - accuracy: 0.4545 - val_loss: 5.8633 - val_accuracy: 0.4909 - lr: 3.0000e-04\n","Epoch 10/150\n","25/25 [==============================] - 1s 41ms/step - loss: 6.6839 - accuracy: 0.4429 - val_loss: 4.5755 - val_accuracy: 0.4864 - lr: 3.0000e-04\n","Epoch 11/150\n","25/25 [==============================] - 1s 44ms/step - loss: 6.7587 - accuracy: 0.4229 - val_loss: 5.7346 - val_accuracy: 0.5409 - lr: 3.0000e-04\n","Epoch 12/150\n","25/25 [==============================] - 1s 46ms/step - loss: 6.6022 - accuracy: 0.4509 - val_loss: 6.2570 - val_accuracy: 0.4773 - lr: 3.0000e-04\n","Epoch 13/150\n","25/25 [==============================] - 1s 46ms/step - loss: 6.5450 - accuracy: 0.4585 - val_loss: 5.6002 - val_accuracy: 0.5318 - lr: 3.0000e-04\n","Epoch 14/150\n","25/25 [==============================] - 1s 38ms/step - loss: 6.5736 - accuracy: 0.4589 - val_loss: 5.9158 - val_accuracy: 0.4568 - lr: 3.0000e-05\n","Epoch 15/150\n","25/25 [==============================] - 1s 38ms/step - loss: 6.6573 - accuracy: 0.4600 - val_loss: 7.2603 - val_accuracy: 0.4000 - lr: 3.0000e-05\n","Epoch 16/150\n","25/25 [==============================] - 1s 36ms/step - loss: 6.2281 - accuracy: 0.4571 - val_loss: 5.5781 - val_accuracy: 0.5136 - lr: 3.0000e-05\n","Epoch 17/150\n","25/25 [==============================] - 1s 38ms/step - loss: 6.3935 - accuracy: 0.4804 - val_loss: 6.1593 - val_accuracy: 0.4159 - lr: 3.0000e-05\n","Epoch 18/150\n","25/25 [==============================] - 1s 36ms/step - loss: 6.4472 - accuracy: 0.4593 - val_loss: 4.8868 - val_accuracy: 0.5295 - lr: 3.0000e-05\n","Epoch 19/150\n","25/25 [==============================] - 1s 38ms/step - loss: 6.3512 - accuracy: 0.4695 - val_loss: 4.6041 - val_accuracy: 0.5523 - lr: 3.0000e-06\n","Epoch 20/150\n","25/25 [==============================] - 1s 37ms/step - loss: 6.2002 - accuracy: 0.4709 - val_loss: 5.4471 - val_accuracy: 0.5045 - lr: 3.0000e-06\n","Epoch 21/150\n","25/25 [==============================] - 1s 36ms/step - loss: 6.9149 - accuracy: 0.4378 - val_loss: 5.2748 - val_accuracy: 0.5114 - lr: 3.0000e-06\n","Epoch 22/150\n","25/25 [==============================] - 1s 37ms/step - loss: 6.4252 - accuracy: 0.4480 - val_loss: 6.8921 - val_accuracy: 0.4545 - lr: 3.0000e-06\n","Epoch 23/150\n","25/25 [==============================] - 1s 38ms/step - loss: 6.4295 - accuracy: 0.4753 - val_loss: 6.5709 - val_accuracy: 0.4523 - lr: 3.0000e-06\n","Epoch 23: early stopping\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"87K6-WF3GtCK","executionInfo":{"status":"ok","timestamp":1688915256871,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vidya","userId":"07181660108449302432"}}},"execution_count":null,"outputs":[]}]}